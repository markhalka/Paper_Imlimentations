{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evolutionary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNe/ldvCJ4V+sB9l4jcuk6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markhalka/Paper_Imlimentations/blob/main/CME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1jbMj32ccFc"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import multiprocessing\n",
        "\n",
        "\"\"\"\n",
        "A simple cross entropy method for optomizing a reinforcement learning task\n",
        "\"\"\"\n",
        "\n",
        "MAX_STEPS = 100\n",
        "N_TEST = 2\n",
        "\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
        "nS = env.observation_space.n\n",
        "nA = env.action_space.n\n",
        "weights_dim = nS*nA\n",
        "n_weights = 10\n",
        "\n",
        "def collect_samples():\n",
        "    mean = 0.0       \n",
        "    std = 1.0\n",
        "    weights = [mean + std*np.random.randn(weights_dim) for i_weight in range(n_weights)]\n",
        "    return weights\n",
        "\n",
        "def get_best_action(state, weight):\n",
        "    return np.argmax([weight[state*nA + action] for action in range(nA)])\n",
        "\n",
        "def get_action(state, weight):\n",
        "    weights = [weight[state * nA + a] for a in range(nA)]\n",
        "    weights = F.softmax(torch.tensor(weights))\n",
        "    return np.random.choice(np.arange(nA), p=weights)\n",
        "\n",
        "def test(weight):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    total_reward = 0.0\n",
        "    while not done and steps < MAX_STEPS:\n",
        "        steps += 1\n",
        "        action = get_action(state, weight)\n",
        "        next_s, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = env.reset() if done else next_s\n",
        "    return total_reward\n",
        "\n",
        "def evaluate_agent(weight):\n",
        "    total_reward = 0.0\n",
        "    for i in range(N_TEST):\n",
        "        total_reward += test(weight)\n",
        "\n",
        "    return total_reward / N_TEST \n",
        "\n",
        "def find_elites(rewards, weights_pop):\n",
        "    n_elite = 4\n",
        "    elite_idxs = np.array(rewards).argsort()[-n_elite:]\n",
        "    elite_weights = [weights_pop[idx] for idx in elite_idxs]\n",
        "    return elite_weights\n",
        "\n",
        "def fit_distribution(elite_weights):\n",
        "    means = np.zeros(nS*nA)\n",
        "    stds = np.zeros(nS*nA)\n",
        "    n_elite_weights = len(elite_weights)\n",
        "    new_weights = n_weights - n_elite_weights\n",
        "    weights = np.zeros((n_weights, weights_dim))\n",
        "    for i in range(nS):\n",
        "        for j in range(nA):\n",
        "            lst = np.array([elite_weights[k][i*nA + j] for k in range(n_elite_weights)])\n",
        "            mean = lst.mean()\n",
        "            std = lst.std() + 0.5\n",
        "            rand_vals = mean + std*np.random.randn(n_weights)\n",
        "            for k in range(new_weights):\n",
        "                weights[k][i*nA + j] = rand_vals[k]\n",
        "    for i in range(n_elite_weights):\n",
        "        weights[i+new_weights] = elite_weights[i]\n",
        "\n",
        "    return weights\n",
        "\n",
        "def pretty_print(weights):\n",
        "    new_w = np.ones((4,4), dtype=np.float32)\n",
        "    print(weights)\n",
        "    for s in range(16):\n",
        "        best_a = np.argmax([weights[s*nA+a] for a in range(nA)])\n",
        "        row = s // 4\n",
        "        col = s % 4\n",
        "        new_w[row,col] = best_a\n",
        "    print(new_w)\n",
        "\n",
        "\n",
        "def run():\n",
        "    weights = collect_samples()\n",
        "    with multiprocessing.Pool(10) as p:\n",
        "        for i in range(10000):\n",
        "            rewards = p.map(evaluate_agent, [w for w in weights])\n",
        "            if i % 100 == 0:\n",
        "                print(\"iter {0}, max_reward: {1}\".format(i, np.max(rewards)))\n",
        "                pretty_print(weights[0])\n",
        "\n",
        "            elites = find_elites(rewards, weights)\n",
        "            weights = fit_distribution(elites)\n",
        "\n",
        "run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}