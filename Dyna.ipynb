{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dyna.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoxEsu+1YcG7RX/T3aTrXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markhalka/Paper_Imlimentations/blob/main/Dyna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X45QjNT1WPk2"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from queue import PriorityQueue\n",
        "\n",
        "\"\"\"\n",
        "A work-in-progress implimentation of \"Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping\" by Sutton et. al\n",
        "\"\"\"\n",
        "\n",
        "# define parameters as in the paper:\n",
        "EPS = 0.3\n",
        "GAMMA = 1\n",
        "LR = 0.01\n",
        "MODEL_LR = 0.1\n",
        "MAX_STEPS = 300\n",
        "FEAT_SIZE = 16\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "class Dyna():\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.act_size = self.env.action_space.n\n",
        "        self.obs_size = self.env.observation_space.n\n",
        "        self.state = self.env.reset()\n",
        "        self.eps = EPS\n",
        "        self.gamma = GAMMA\n",
        "        self.lr = LR\n",
        "        self.model_lr = MODEL_LR\n",
        "        self.model = defaultdict(tuple)\n",
        "        self.b = np.zeros((self.act_size, self.obs_size))\n",
        "        self.F = np.zeros((self.act_size, self.obs_size, self.obs_size))\n",
        "        self.q_vals = np.zeros((self.obs_size, self.act_size))\n",
        "        self.MAX_STEPS = MAX_STEPS\n",
        "        self.queue = PriorityQueue(maxsize=100)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        if np.random.random() < self.eps:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return self.get_best_action(state)\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        return np.argmax([self.q_vals[state, i] for i in range(self.act_size)])\n",
        "\n",
        "    def get_feat(self, state):\n",
        "        feat = np.zeros(FEAT_SIZE)\n",
        "        feat[state] = 1\n",
        "        return feat\n",
        "\n",
        "    def update_q_vals(self, state, action, next_s, reward):\n",
        "        td_error = reward + self.gamma * self.q_vals[next_s, self.get_best_action(next_s)] - self.q_vals[state, action]\n",
        "        self.q_vals[state, action] += self.lr * td_error\n",
        "        if abs(td_error) > 0.001 and not self.queue.full():\n",
        "            self.queue.put(state, td_error)\n",
        "\n",
        "    def update_F(self, next_feat, feat, action):\n",
        "        self.F[action] = self.F[action] + self.model_lr * np.matmul((next_feat - np.matmul(self.F[action], feat)), feat.T)\n",
        "\n",
        "    def update_b(self, r, feat, action):\n",
        "        self.b[action] = self.b[action] + self.model_lr * (r - np.matmul(self.b[action].T, feat)) * feat\n",
        "\n",
        "    def update_model(self, state, action, next_s, reward):\n",
        "        feat = self.get_feat(state)\n",
        "        next_feat = self.get_feat(next_s)\n",
        "        self.update_F(next_feat, feat, action)\n",
        "        self.update_b(reward, feat, action)\n",
        "    \n",
        "    def step(self):\n",
        "        done = False\n",
        "        steps = 0\n",
        "        feat = self.get_feat(self.state)\n",
        "        while not done and steps < self.MAX_STEPS:\n",
        "            steps += 1\n",
        "            action = self.get_action(self.state)\n",
        "            next_s, reward, done, _ = self.env.step(action)\n",
        "            self.update_q_vals(self.state, action, next_s, reward)\n",
        "            self.update_model(self.state, action, next_s, reward)\n",
        "            self.state = self.env.reset() if done else next_s\n",
        "        self.planning(5)\n",
        "\n",
        "    def run_test(self):\n",
        "        self.state = self.env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        total_reward = 0.0\n",
        "        while not done and steps < self.MAX_STEPS:\n",
        "            steps += 1\n",
        "            action = self.get_best_action(self.state)\n",
        "            next_s, reward, done, _ = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            self.state = self.env.reset() if done else next_s\n",
        "        return total_reward\n",
        "\n",
        "    def test(self, n_test):\n",
        "        total_reward = 0.0\n",
        "        for i in range(n_test):\n",
        "            total_reward += self.run_test()\n",
        "        return total_reward / n_test\n",
        "        \n",
        "    def planning(self, planning_steps):\n",
        "        for steps in range(planning_steps):\n",
        "            if self.queue.empty():\n",
        "                return\n",
        "            i = int(self.queue.get())\n",
        "            for j in range(16):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                a = np.argmax(self.F[:, i, j])\n",
        "                if self.F[a, i, j] > 0.01:\n",
        "                    current_a = np.argmax(self.q_vals[i, :])\n",
        "                    max_q = np.max(self.q_vals[j, :])\n",
        "                    td_error = np.max(self.b[:, i] + self.gamma * self.F[:, i, j] * max_q) - self.q_vals[i, current_a]\n",
        "                    self.q_vals[i, current_a] += self.lr * td_error\n",
        "                    if not self.queue.full() and abs(td_error) > 1e-2:\n",
        "                        self.queue.put(j, td_error)\n",
        "\n",
        "agent = Dyna()\n",
        "\n",
        "for i in range(100000):\n",
        "    agent.step()\n",
        "    if i % 1000 == 0:\n",
        "        print(agent.test(10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}