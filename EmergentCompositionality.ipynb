{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmergentCompositionality.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpQ2vm/wGm77SNQwR95qYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markhalka/Paper_Imlimentations/blob/main/EmergentCompositionality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twPcnbvIWVWk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# what happens if you have less outputs than actions? (half as many?, one tenth?)\n",
        "# and maybe the reciever has some primitive short term memory\n",
        "# what kind of results would there be?\n",
        "\n",
        "# you could easily update this to inlucde an array of senders and recievers for arbitrary depth\n",
        "# (and you could also make it arbitrarily wide as well..)\n",
        "\n",
        "\"\"\"\n",
        "Implimentation of: CREATIVE COMPOSITIONALITY FROM REINFORCEMENT LEARNING IN SIGNALING GAMES by Michael Franke\n",
        "\n",
        "the general idea, is that creative compositionality can be explained by two mechanisms: spill-over, and lateral inhibition (both biologically plausible)\n",
        "\"\"\"\n",
        "\n",
        "# use the same default parameters as the paper\n",
        "DEFUALT_SIMILARITY = 0.05\n",
        "DEFAULT_INHIBITION = 0.2\n",
        "class Agent():\n",
        "    def __init__(self, states, name):\n",
        "        self.states = states\n",
        "        self.name = name\n",
        "        self.n_states = len(states)\n",
        "        self.counts = np.ones((self.n_states, self.n_states))\n",
        "        self.last_input = None\n",
        "        self.last_ouput = None\n",
        "        self.i = DEFAULT_INHIBITION\n",
        "        self.s = DEFAULT_SIMILARITY\n",
        "    \n",
        "    def get_index(self, state):\n",
        "        return self.states.index(state)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        probs = []\n",
        "        base_prob = 0.0\n",
        "        index = self.get_index(state)\n",
        "        for i in range(self.n_states):\n",
        "            base_prob +=  self.counts[index][i]\n",
        "            probs.append(self.counts[index][i])\n",
        "        probs = probs / base_prob\n",
        "        output_index = np.random.choice(np.arange(self.n_states), p=probs)\n",
        "        output = self.states[output_index]\n",
        "        self.last_input = state\n",
        "        self.last_output = output\n",
        "        return output\n",
        "\n",
        "    def get_sim(self, state1, state2):\n",
        "        count = 0\n",
        "        state1, state2 = (state1, state2) if len(state1) > len(state2) else (state2, state1)\n",
        "        for i in range(len(state2)):\n",
        "            was_found = False\n",
        "            for j in range(len(state1)):\n",
        "                if state1[j] == state2[i]:\n",
        "                    was_found = True\n",
        "                    break\n",
        "            if was_found:\n",
        "                count += 1\n",
        "        return count / len(state1)\n",
        "\n",
        "    def update_spill_over(self, reward):\n",
        "        for i, input in enumerate(self.states, 0):\n",
        "            for o, output in enumerate(self.states, 0):\n",
        "                input_sim = self.get_sim(self.last_input, input)\n",
        "                output_sim = self.get_sim(self.last_output, output)\n",
        "                total_sim = input_sim * output_sim\n",
        "                if total_sim != 1:\n",
        "                    total_sim *= self.s\n",
        "                self.counts[i][o] += total_sim * reward\n",
        "\n",
        "    def update_lateral(self, reward):\n",
        "        output_index = self.get_index(self.last_output)\n",
        "        input_index = self.get_index(self.last_input)\n",
        "        for s, state in enumerate(self.states, 0):\n",
        "            if state != self.last_input:\n",
        "                self.counts[s][output_index] = max(self.counts[s][output_index] - self.i, 0)\n",
        "            if state != self.last_output:\n",
        "                self.counts[input_index][s] = max(self.counts[input_index][s] - self.i, 0)\n",
        "\n",
        "    def update_counts(self, reward):\n",
        "        if reward == 0:\n",
        "            return   \n",
        "        input_index = self.get_index(self.last_input)\n",
        "        output_index = self.get_index(self.last_output)\n",
        "        self.update_spill_over(reward)\n",
        "        self.update_lateral(reward)\n",
        "\n",
        "\n",
        "class Env():\n",
        "    def __init__(self, states):\n",
        "        self.states = states\n",
        "        self.n_states = len(states)\n",
        "        self.current_state = None\n",
        "        \n",
        "    def get_state(self):\n",
        "        index = np.random.randint(self.n_states)\n",
        "        self.current_state = self.states[index]\n",
        "        return self.current_state\n",
        "    \n",
        "    def check_state(self, state):\n",
        "        return int(self.current_state == state)\n",
        "\n",
        "\n",
        "class Communication():\n",
        "    def __init__(self):\n",
        "        self.states = [\"a\",\"b\",\"c\",\"ab\",\"ac\",\"bc\"]\n",
        "        self.env = Env(self.states)\n",
        "        self.sender = Agent(self.states, \"send\")\n",
        "        self.reciever = Agent(self.states, \"rec\")\n",
        "    \n",
        "    def step(self):\n",
        "        state = self.env.get_state()\n",
        "        sender_state = self.sender.get_action(state)\n",
        "        reciever_state = self.reciever.get_action(sender_state)\n",
        "        reward = self.env.check_state(reciever_state)\n",
        "        self.sender.update_counts(reward)\n",
        "        self.reciever.update_counts(reward)\n",
        "        return reward\n",
        "\n",
        "    def test(self):\n",
        "        print(self.sender.get_sim(\"c\",\"ab\"))\n",
        "\n",
        "    def run(self):\n",
        "        total_reward = 0.0\n",
        "        for i in range(10000):\n",
        "            total_reward += self.step()\n",
        "            if i % 1000 == 0:\n",
        "                print(total_reward / 1000.0)\n",
        "                total_reward = 0.0\n",
        "        print(self.sender.counts)\n",
        "        print(\"\\n\")\n",
        "        print(self.reciever.counts)\n",
        "\n",
        "comm = Communication()\n",
        "comm.run()\n",
        "#comm.test()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}